# -*- coding: utf-8 -*-
"""EcocnomÃ­a Circular.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TLsH40EaVazpxNndR8h2ugCyffLoDXsZ
"""

# app_allocation_pitch.py
"""
MiPyME Credit Allocation (Pitch-ready)
- Entrena una MLP pequeÃ±a en datos sintÃ©ticos (rÃ¡pido, cacheado)
- Predice probabilidad de acceso/repago
- Asigna un nÃºmero limitado de crÃ©ditos optimizando probabilidad + IIC + equidad por polos/estados
- UI rÃ¡pida y visual para pitch
Requisitos: streamlit, pandas, numpy, scikit-learn, plotly
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, accuracy_score

st.set_page_config(layout="wide", page_title="AsignaciÃ³n Eficiente de CrÃ©ditos - Pitch")
st.title("AsignaciÃ³n eficiente de crÃ©ditos a MiPyMEs â€” (Pitch-ready)")

# ----------------------------
# 1) Datos sintÃ©ticos (cacheados)
# ----------------------------
@st.cache_data
def generate_synthetic(n=800, seed=42):
    np.random.seed(seed)
    estados = ['CDMX','Jalisco','Nuevo LeÃ³n','Puebla','Oaxaca','Veracruz','Chiapas','YucatÃ¡n','Guanajuato','Baja California']
    polos = {
        'Centro': ['CDMX','Puebla'],
        'Occidente': ['Jalisco','Guanajuato'],
        'Norte': ['Nuevo LeÃ³n','Baja California'],
        'Sureste': ['Oaxaca','Chiapas','YucatÃ¡n'],
        'Golfo': ['Veracruz']
    }
    sectores = ['Manufactura','Comercio','Servicios','Agro']
    df = pd.DataFrame({
        'id': np.arange(n),
        'estado': np.random.choice(estados, n, p=None),
        'sector': np.random.choice(sectores, n),
        'ventas': np.round(np.random.lognormal(mean=10, sigma=1, size=n)),
        'empleados': np.random.poisson(5, n),
        'lider_mujer': np.random.binomial(1, 0.35, n),
        'recicla_pct': np.round(np.random.beta(2,5,n),2),
        'digital_score': np.round(np.random.beta(2,2,n),2),
        'factura_e': np.random.binomial(1, 0.5, n),
        'ventas_online_pct': np.round(np.random.beta(1.5,4,n),2),
    })
    # map estado -> polo
    inv_map = {}
    for p, states in polos.items():
        for s in states:
            inv_map[s] = p
    df['polo'] = df['estado'].map(lambda x: inv_map.get(x, 'Otros'))
    # construct target 'credito_actual' as noisy function
    base_prob = (
        0.25*df['digital_score'] +
        0.20*df['factura_e'] +
        0.15*df['recicla_pct'] +
        0.10*(np.log1p(df['ventas'])/np.log1p(df['ventas']).max()) +
        0.10*(df['empleados']/ (df['empleados'].max()+1)) +
        0.10*df['lider_mujer']
    )
    noise = np.random.normal(0, 0.05, size=n)
    prob = np.clip(base_prob + noise, 0, 1)
    df['prob_credit_true'] = prob
    df['credito_actual'] = (prob > np.quantile(prob, 0.68)).astype(int)
    return df

df_base = generate_synthetic()

# ----------------------------
# 2) Entrenamiento rÃ¡pido de la MLP (cache_resource para no reentrenar)
# ----------------------------
@st.cache_resource
def train_light_mlp(df, random_state=42):
    features = ['ventas','empleados','recicla_pct','digital_score','factura_e','ventas_online_pct','lider_mujer']
    X = df[features].copy()
    X['ventas_log'] = np.log1p(X['ventas'])
    X = X.drop(columns=['ventas'])
    y = df['credito_actual']
    # quick train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=random_state)
    scaler = StandardScaler()
    X_train_s = scaler.fit_transform(X_train)
    X_test_s = scaler.transform(X_test)
    clf = MLPClassifier(hidden_layer_sizes=(24,12), max_iter=150, early_stopping=True, random_state=random_state)
    clf.fit(X_train_s, y_train)
    y_proba = clf.predict_proba(X_test_s)[:,1]
    auc = roc_auc_score(y_test, y_proba)
    acc = accuracy_score(y_test, (y_proba>0.5).astype(int))
    return scaler, clf, auc, acc

scaler, clf, auc, acc = train_light_mlp(df_base)
st.sidebar.markdown(f"**Modelo entrenado (rÃ¡pido)** â€” AUC: {auc:.3f}, Acc: {acc:.3f}")

# ----------------------------
# 3) UI: parÃ¡metros del pitch / restricciones de asignaciÃ³n
# ----------------------------
st.sidebar.header("ParÃ¡metros de asignaciÃ³n (Pitch)")
budget = st.sidebar.number_input("CrÃ©ditos disponibles (cupos)", min_value=1, max_value=500, value=40)
alpha = st.sidebar.slider("Peso: Probabilidad de repago (modelo)", 0.0, 1.0, 0.6)    # importance of model probability
beta = st.sidebar.slider("Peso: EconomÃ­a circular (IIC)", 0.0, 1.0, 0.25)
gamma = st.sidebar.slider("Peso: Prioridad por polos (favorecer polos objetivos)", 0.0, 1.0, 0.1)
gender_boost = st.sidebar.slider("Bono a empresas lideradas por mujeres (0-0.4)", 0.0, 0.4, 0.05)

# constraint: max share per state to enforce geographic spread
max_share_state = st.sidebar.slider("MÃ¡x % por estado (para evitar concentraciÃ³n)", 0.05, 0.50, 0.20)

st.sidebar.markdown("---")
st.sidebar.markdown("ðŸ’¡ Consejo: para el pitch, mantÃ©n el budget pequeÃ±o (20-60) y muestra cÃ³mo la polÃ­tica cambia la asignaciÃ³n.")

# ----------------------------
# 4) Prepare data, compute IIC and model scores
# ----------------------------
df = df_base.copy()
# IIC: composite index for circular integration (0-1)
df['score_circular'] = (df['recicla_pct'] + df['ventas_online_pct'])/2
df['score_digital'] = df['digital_score']
df['score_formal'] = df['factura_e']
df['score_gender'] = df['lider_mujer']
w_circ, w_dig, w_form, w_gen = 0.4, 0.3, 0.2, 0.1
df['IIC'] = (w_circ*df['score_circular'] + w_dig*df['score_digital'] + w_form*df['score_formal'] + w_gen*df['score_gender'])

# Model probability (inference)
def predict_proba_df(df, scaler, clf):
    X = df[['ventas','empleados','recicla_pct','digital_score','factura_e','ventas_online_pct','lider_mujer']].copy()
    X['ventas_log'] = np.log1p(X['ventas'])
    X = X.drop(columns=['ventas'])
    X_s = scaler.transform(X)
    return clf.predict_proba(X_s)[:,1]

df['model_prob'] = predict_proba_df(df, scaler, clf)

# ----------------------------
# 5) Priority score & allocation algorithm (greedy + constraints)
# ----------------------------
# compute polo priority factor: user may highlight some polos (for pitch we compute a simple boosting for 'Centro' and 'Occidente')
priority_polos_default = st.sidebar.multiselect("Polo(s) prioritarios (boost)", options=df['polo'].unique().tolist(), default=['Centro'])
polo_boost_map = {p: (1.0 + 0.15) if p in priority_polos_default else 1.0 for p in df['polo'].unique()}

df['polo_boost'] = df['polo'].map(polo_boost_map)

# compute combined score (normalized)
# normalize fields to 0-1
def minmax(series):
    if series.max() == series.min():
        return series*0.0
    return (series - series.min())/(series.max()-series.min())

df['s_model'] = minmax(df['model_prob'])
df['s_iic'] = minmax(df['IIC'])
df['s_polo'] = minmax(df['polo_boost'])  # polo boost is small discrete
df['s_gender'] = df['lider_mujer']  # already 0/1

# combined priority score
df['priority_score'] = (
    alpha * df['s_model'] +
    beta * df['s_iic'] +
    gamma * df['s_polo'] +
    gender_boost * df['s_gender']
)

# Greedy allocation with per-state cap
def allocate_greedy(df, budget, max_share_state):
    # copy and sort by score desc
    df_sorted = df.sort_values('priority_score', ascending=False).copy().reset_index(drop=True)
    allocation = []
    state_counts = {}
    max_per_state = max(1, int(np.ceil(budget * max_share_state)))
    for idx, row in df_sorted.iterrows():
        st_name = row['estado']
        if len(allocation) >= budget:
            break
        if state_counts.get(st_name, 0) >= max_per_state:
            continue  # skip to avoid concentration
        allocation.append(row['id'])
        state_counts[st_name] = state_counts.get(st_name, 0) + 1
    # If we didn't fill budget due to strict caps, relax caps and fill by score
    if len(allocation) < budget:
        remaining = df_sorted[~df_sorted['id'].isin(allocation)]
        needed = budget - len(allocation)
        allocation += remaining['id'].tolist()[:needed]
    return allocation

selected_ids = allocate_greedy(df, budget, max_share_state)

df['selected'] = df['id'].isin(selected_ids).astype(int)

# ----------------------------
# 6) Output & Visuals
# ----------------------------
st.subheader("ðŸ”Ž Resumen y visualizaciÃ³n")

colA, colB = st.columns([2,1])

with colA:
    fig = px.histogram(df, x='priority_score', nbins=30, title='DistribuciÃ³n de prioridad (score compuesto)')
    st.plotly_chart(fig, use_container_width=True)
    fig2 = px.scatter(df, x='model_prob', y='IIC', color='selected', hover_data=['estado','polo','sector'], title='Probabilidad (modelo) vs IIC (seleccionados en color)')
    st.plotly_chart(fig2, use_container_width=True)

with colB:
    st.metric("CrÃ©ditos disponibles", f"{budget}")
    st.metric("CrÃ©ditos asignados", f"{df['selected'].sum()}")
    # breakdown by state for selected
    sel_by_state = df[df['selected']==1].groupby('estado').agg({'id':'count'}).rename(columns={'id':'asignados'}).reset_index()
    st.table(sel_by_state.sort_values('asignados', ascending=False).head(10))

st.subheader("ðŸ“‹ Tabla de asignaciÃ³n (top seleccionados)")
display_cols = ['id','estado','polo','sector','ventas','empleados','lider_mujer','IIC','model_prob','priority_score','selected']
st.dataframe(df.sort_values('priority_score', ascending=False)[display_cols].head(50))

# ----------------------------
# 7) Metrics to show impact & fairness (for pitch)
# ----------------------------
st.subheader("ðŸ“ MÃ©tricas de impacto y equidad (pitch)")
total_selected = df['selected'].sum()
avg_prob_selected = df.loc[df['selected']==1, 'model_prob'].mean()
avg_IIC_selected = df.loc[df['selected']==1, 'IIC'].mean()
women_share_selected = df.loc[df['selected']==1, 'lider_mujer'].mean()

col1, col2, col3, col4 = st.columns(4)
col1.metric("Prom prob (selected)", f"{avg_prob_selected:.2%}")
col2.metric("Prom IIC (selected)", f"{avg_IIC_selected:.3f}")
col3.metric("% lideradas por mujeres (selected)", f"{women_share_selected:.2%}")
col4.metric("Estados representados", f"{df.loc[df['selected']==1,'estado'].nunique()}")

# ----------------------------
# 8) Export selection (CSV)
# ----------------------------
st.subheader("ðŸ’¾ Exportar selecciÃ³n")
if st.button("Descargar CSV con asignaciÃ³n"):
    to_export = df[df['selected']==1].copy()
    to_export = to_export[display_cols]
    csv = to_export.to_csv(index=False).encode('utf-8')
    st.download_button(label="Descargar CSV", data=csv, file_name="asignacion_creditos_selected.csv", mime="text/csv")

# ----------------------------
# 9) Notes for the pitch
# ----------------------------
st.markdown("---")
st.markdown("**Notas para el pitch (hablar rÃ¡pido):**")
st.markdown("""
- El modelo predice probabilidad de acceso/restituciÃ³n de crÃ©dito (inferido con red neuronal entrenada offline en datos histÃ³ricos sintÃ©ticos/administrativos).
- La asignaciÃ³n optimiza una combinaciÃ³n: **probabilidad de repago (modelo) + mÃ©rito de economÃ­a circular (IIC) + prioridad por polos + perspectiva de gÃ©nero**.
- Para evitar concentraciÃ³n, aplicamos un **lÃ­mite por estado** (parÃ¡metro `MÃ¡x % por estado`) que garantiza dispersiÃ³n geogrÃ¡fica y fomenta polos locales.
- En producciÃ³n usarÃ­amos el mismo pipeline: **entrenamiento offline** + **modelo cargado en inferencia**. AquÃ­ demostramos la lÃ³gica completa y las mÃ©tricas en < 5 s.
""")

st.caption("Prototipo rÃ¡pido: para producciÃ³n se recomienda validar con datos reales, auditar sesgos y realizar optimizaciÃ³n entera (programaciÃ³n lineal entera) para restricciones complejas.")